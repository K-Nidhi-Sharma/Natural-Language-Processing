{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cded88c5",
   "metadata": {},
   "source": [
    "# <center> NATURAL LANGUAGE PROCESSING (NLP) </center>\n",
    "### <center> K NIDHI SHARMA </center>\n",
    "### <center> VECTORIZATION </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d57022",
   "metadata": {},
   "source": [
    "Vectorization is a methodology to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.\n",
    "\n",
    "The process of converting words into numbers are called Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a71e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa46706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MDS373A_NLP_syllabus.txt',\n",
       " 'da.txt',\n",
       " 'ds.txt',\n",
       " 'mca.txt',\n",
       " 'newsemi.txt',\n",
       " 'stat.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = \"E:\\\\PG_sem3\\\\nlp\\\\corpus\"\n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root,'.*')\n",
    "lst = wordlists.fileids()\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f76115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MDS373A_NLP_syllabus.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05aca3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MDS373A', ':', 'Natural', 'Language', 'Processing', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=wordlists.words()\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89727abb",
   "metadata": {},
   "source": [
    "### Count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba3d5e6",
   "metadata": {},
   "source": [
    "Count vectorizer is a method to convert text to numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e85a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ad0f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['05', '10', '106101007', '12', '150', '1994', '1999', '2009', '2013', '2014', '2022', '2nd', '30', '50', '90', 'above', 'abroad', 'ac', 'achieve', 'additional', 'advantage', 'aggregate', 'aiu', 'algorithms', 'all', 'also', 'alternative', 'always', 'ambiguity', 'among', 'amongst', 'an', 'analysis', 'analytics', 'analyze', 'and', 'antonyms', 'any', 'appearing', 'applicants', 'applications', 'apply', 'approach', 'approaches', 'architecture', 'are', 'art', 'as', 'at', 'audio', 'available', 'avoid', 'bachelor', 'background', 'based', 'bayes', 'bba', 'bca', 'bcom', 'be', 'become', 'been', 'being', 'below', 'between', 'bird', 'bottlenecks', 'bsc', 'business', 'by', 'cambridge', 'came', 'can', 'candidate', 'candidates', 'case', 'characteristics', 'check', 'chemistry', 'chromosomes', 'circle', 'class', 'classes', 'classical', 'classification', 'classifications', 'co1', 'co2', 'co3', 'coherence', 'cohesion', 'communication', 'comparable', 'computational', 'compute', 'computer', 'concepts', 'conducted', 'considered', 'containing', 'contrastive', 'correction', 'count', 'counterfeit', 'course', 'courses', 'covers', 'credits', 'criteria', 'daniel', 'data', 'deep', 'degree', 'demand', 'deployed', 'described', 'design', 'detecting', 'detection', 'determines', 'dialogue', 'different', 'differentiate', 'disambiguation', 'discourse', 'distance', 'doing', 'due', 'ed3book', 'edit', 'edition', 'edu', 'edward', 'efficiency', 'efficient', 'efficiently', 'either', 'eligible', 'embeddings', 'emphasizing', 'employed', 'english', 'error', 'essential', 'estimation', 'evaluating', 'evaluation', 'even', 'ewan', 'examination', 'examinations', 'exceptional', 'exercises', 'exhibit', 'expressions', 'familiar', 'famous', 'far', 'features', 'few', 'fewer', 'field', 'fields', 'final', 'finding', 'following', 'for', 'forgery', 'foundations', 'frequency', 'from', 'fulfill', 'functions', 'generated', 'generation', 'get', 'glove', 'goal', 'graduate', 'gram', 'grams', 'graphical', 'great', 'hall', 'handwritten', 'hard', 'has', 'hausser', 'have', 'having', 'hence', 'heterogeneous', 'hours', 'http', 'https', 'human', 'ibm', 'identical', 'ii', 'image', 'imbalanced', 'in', 'inaccurate', 'including', 'india', 'indian', 'information', 'input', 'inputs', 'instances', 'introduced', 'introduction', 'is', 'it', 'james', 'jurafsky', 'karyotyping', 'kinship', 'kit', 'klein', 'knowledge', 'lab', 'labelled', 'labelling', 'lack', 'language', 'languages', 'last', 'learning', 'lemmatizing', 'level', 'lexical', 'linguistics', 'loper', 'loss', 'ma', 'machine', 'mainly', 'major', 'make', 'many', 'march', 'marks', 'martin', 'massive', 'mathematics', 'maths', 'max', 'may', 'mds373a', 'meaning', 'measure', 'media', 'methodologies', 'methods', 'minimum', 'minor', 'mirrored', 'mit', 'model', 'modelling', 'models', 'more', 'must', 'naive', 'natural', 'naturallanguage', 'negative', 'network', 'networks', 'neural', 'nlg', 'nlp', 'nltk', 'no', 'non', 'normalization', 'not', 'nptel', 'objectives', 'of', 'on', 'or', 'order', 'org', 'otherwise', 'outcomes', 'output', 'overview', 'parameters', 'parsing', 'part', 'passed', 'pdf', 'performing', 'perspective', 'pharmacology', 'picture', 'poorly', 'portals', 'pos', 'positive', 'pre', 'prentice', 'press', 'problem', 'problems', 'processing', 'profiles', 'program', 'programmes', 'projected', 'python', 'questions', 'reading', 'real', 'recognised', 'recognition', 'recognize', 'recommended', 'reference', 'regular', 'reilly', 'remove', 'representation', 'representations', 'require', 'requiring', 'resolution', 'resources', 'results', 'retrieval', 'roland', 'same', 'samples', 'scale', 'science', 'secured', 'segmentation', 'semantic', 'semantics', 'semester', 'semesters', 'sense', 'sentiment', 'sequence', 'several', 'should', 'siamese', 'signal', 'similar', 'similarity', 'since', 'slp3', 'small', 'smoothed', 'so', 'specialization', 'speech', 'spelling', 'springer', 'stanford', 'state', 'statistical', 'statistics', 'stemming', 'steven', 'stop', 'striking', 'structure', 'students', 'studied', 'studies', 'study', 'sub', 'subjects', 'such', 'summarization', 'supervised', 'synonyms', 'syntactic', 'syntax', 'systems', 'tagging', 'tandem', 'teaching', 'tech', 'technique', 'techniques', 'text', 'the', 'their', 'then', 'these', 'they', 'this', 'though', 'thought', 'time', 'times', 'to', 'tokenize', 'tons', 'tool', 'total', 'train', 'translation', 'triplet', 'twin', 'two', 'ugc', 'under', 'undergraduate', 'understand', 'unique', 'unit', 'university', 'unsupervised', 'use', 'used', 'using', 'valuation', 'various', 'vectors', 'verification', 'web', 'weights', 'whether', 'which', 'who', 'widely', 'will', 'with', 'wonder', 'word', 'word2vec', 'wordnet', 'words', 'working', 'works', 'write', 'www', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(w)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b0ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['05', '10', '106101007', '12', '150', '1994', '1999', '2009', '2013', '2014', '2022', '2nd', '30', '50', '90', 'abroad', 'ac', 'achieve', 'additional', 'advantage', 'aggregate', 'aiu', 'algorithms', 'alternative', 'ambiguity', 'analysis', 'analytics', 'analyze', 'antonyms', 'appearing', 'applicants', 'applications', 'apply', 'approach', 'approaches', 'architecture', 'art', 'audio', 'available', 'avoid', 'bachelor', 'background', 'based', 'bayes', 'bba', 'bca', 'bcom', 'bird', 'bottlenecks', 'bsc', 'business', 'cambridge', 'came', 'candidate', 'candidates', 'case', 'characteristics', 'check', 'chemistry', 'chromosomes', 'circle', 'class', 'classes', 'classical', 'classification', 'classifications', 'co1', 'co2', 'co3', 'coherence', 'cohesion', 'communication', 'comparable', 'computational', 'compute', 'computer', 'concepts', 'conducted', 'considered', 'containing', 'contrastive', 'correction', 'count', 'counterfeit', 'course', 'courses', 'covers', 'credits', 'criteria', 'daniel', 'data', 'deep', 'degree', 'demand', 'deployed', 'described', 'design', 'detecting', 'detection', 'determines', 'dialogue', 'different', 'differentiate', 'disambiguation', 'discourse', 'distance', 'doing', 'ed3book', 'edit', 'edition', 'edu', 'edward', 'efficiency', 'efficient', 'efficiently', 'eligible', 'embeddings', 'emphasizing', 'employed', 'english', 'error', 'essential', 'estimation', 'evaluating', 'evaluation', 'ewan', 'examination', 'examinations', 'exceptional', 'exercises', 'exhibit', 'expressions', 'familiar', 'famous', 'far', 'features', 'fewer', 'field', 'fields', 'final', 'finding', 'following', 'forgery', 'foundations', 'frequency', 'fulfill', 'functions', 'generated', 'generation', 'glove', 'goal', 'graduate', 'gram', 'grams', 'graphical', 'great', 'hall', 'handwritten', 'hard', 'hausser', 'having', 'heterogeneous', 'hours', 'http', 'https', 'human', 'ibm', 'identical', 'ii', 'image', 'imbalanced', 'inaccurate', 'including', 'india', 'indian', 'information', 'input', 'inputs', 'instances', 'introduced', 'introduction', 'james', 'jurafsky', 'karyotyping', 'kinship', 'kit', 'klein', 'knowledge', 'lab', 'labelled', 'labelling', 'lack', 'language', 'languages', 'learning', 'lemmatizing', 'level', 'lexical', 'linguistics', 'loper', 'loss', 'ma', 'machine', 'mainly', 'major', 'make', 'march', 'marks', 'martin', 'massive', 'mathematics', 'maths', 'max', 'mds373a', 'meaning', 'measure', 'media', 'methodologies', 'methods', 'minimum', 'minor', 'mirrored', 'mit', 'model', 'modelling', 'models', 'naive', 'natural', 'naturallanguage', 'negative', 'network', 'networks', 'neural', 'nlg', 'nlp', 'nltk', 'non', 'normalization', 'nptel', 'objectives', 'order', 'org', 'outcomes', 'output', 'overview', 'parameters', 'parsing', 'passed', 'pdf', 'performing', 'perspective', 'pharmacology', 'picture', 'poorly', 'portals', 'pos', 'positive', 'pre', 'prentice', 'press', 'problem', 'problems', 'processing', 'profiles', 'program', 'programmes', 'projected', 'python', 'questions', 'reading', 'real', 'recognised', 'recognition', 'recognize', 'recommended', 'reference', 'regular', 'reilly', 'remove', 'representation', 'representations', 'require', 'requiring', 'resolution', 'resources', 'results', 'retrieval', 'roland', 'samples', 'scale', 'science', 'secured', 'segmentation', 'semantic', 'semantics', 'semester', 'semesters', 'sense', 'sentiment', 'sequence', 'siamese', 'signal', 'similar', 'similarity', 'slp3', 'small', 'smoothed', 'specialization', 'speech', 'spelling', 'springer', 'stanford', 'state', 'statistical', 'statistics', 'stemming', 'steven', 'stop', 'striking', 'structure', 'students', 'studied', 'studies', 'study', 'sub', 'subjects', 'summarization', 'supervised', 'synonyms', 'syntactic', 'syntax', 'systems', 'tagging', 'tandem', 'teaching', 'tech', 'technique', 'techniques', 'text', 'thought', 'time', 'times', 'tokenize', 'tons', 'tool', 'total', 'train', 'translation', 'triplet', 'twin', 'ugc', 'undergraduate', 'understand', 'unique', 'unit', 'university', 'unsupervised', 'use', 'used', 'using', 'valuation', 'various', 'vectors', 'verification', 'web', 'weights', 'widely', 'wonder', 'word', 'word2vec', 'wordnet', 'words', 'working', 'works', 'write', 'www', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "## Stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(w[])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8067d8",
   "metadata": {},
   "source": [
    "X.toarray() will display output in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8e6b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>106101007</th>\n",
       "      <th>12</th>\n",
       "      <th>150</th>\n",
       "      <th>1994</th>\n",
       "      <th>1999</th>\n",
       "      <th>2009</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>words</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>write</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      05  10  106101007  12  150  1994  1999  2009  2013  2014  ...  word  \\\n",
       "0      0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "1      0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "2      0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "3      0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "4      0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "...   ..  ..        ...  ..  ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "1425   0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "1426   0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "1427   0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "1428   0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "1429   0   0          0   0    0     0     0     0     0     0  ...     0   \n",
       "\n",
       "      word2vec  wordnet  words  working  works  write  www  year  years  \n",
       "0            0        0      0        0      0      0    0     0      0  \n",
       "1            0        0      0        0      0      0    0     0      0  \n",
       "2            0        0      0        0      0      0    0     0      0  \n",
       "3            0        0      0        0      0      0    0     0      0  \n",
       "4            0        0      0        0      0      0    0     0      0  \n",
       "...        ...      ...    ...      ...    ...    ...  ...   ...    ...  \n",
       "1425         0        0      0        0      0      0    0     0      0  \n",
       "1426         0        0      0        0      0      0    0     0      0  \n",
       "1427         0        0      0        0      0      0    0     0      0  \n",
       "1428         0        0      0        0      0      0    0     0      0  \n",
       "1429         0        0      0        0      0      0    0     0      0  \n",
       "\n",
       "[1430 rows x 378 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc_Term_Matrix = pd.DataFrame(X.toarray(),columns= vectorizer.get_feature_names())\n",
    "Doc_Term_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6424b45",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699e43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacd919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mds373a': 213, 'natural': 227, 'language': 192, 'processing': 262, 'total': 345, 'teaching': 334, 'hours': 162, 'semester': 295, '90': 14, 'max': 212, 'marks': 207, '150': 4, 'credits': 87, '05': 0, 'course': 84, 'objectives': 239, 'goal': 150, 'make': 205, 'familiar': 132, 'concepts': 76, 'study': 323, 'human': 165, 'computational': 73, 'perspective': 250, 'covers': 86, 'syntactic': 329, 'semantic': 293, 'discourse': 104, 'models': 225, 'emphasizing': 117, 'machine': 202, 'learning': 194, 'outcomes': 242, 'co1': 66, 'understand': 352, 'various': 361, 'approaches': 34, 'syntax': 330, 'semantics': 294, 'nlp': 234, 'co2': 67, 'apply': 32, 'methods': 218, 'generation': 148, 'dialogue': 100, 'summarization': 326, 'using': 359, 'co3': 68, 'analyze': 27, 'methodologies': 217, 'used': 358, 'translation': 347, 'techniques': 337, 'including': 172, 'unsupervised': 356, 'real': 270, 'time': 340, 'applications': 31, 'unit': 354, '12': 3, 'introduction': 180, 'background': 41, 'overview': 244, 'hard': 158, 'ambiguity': 24, 'algorithms': 22, 'knowledge': 187, 'bottlenecks': 48, 'nltk': 235, 'case': 55, 'parsing': 246, 'word': 368, 'level': 196, 'analysis': 25, 'regular': 276, 'expressions': 131, 'text': 338, 'normalization': 237, 'edit': 108, 'distance': 105, 'spelling': 309, 'error': 120, 'detection': 98, 'correction': 81, 'words': 371, 'classes': 62, 'speech': 308, 'tagging': 332, 'naive': 226, 'bayes': 43, 'sentiment': 298, 'classification': 64, 'smoothed': 306, 'estimation': 122, 'modelling': 224, 'gram': 152, 'grams': 153, 'evaluating': 123, 'problem': 260, 'meaning': 214, 'representation': 279, 'lexical': 197, 'sense': 297, 'disambiguation': 103, 'cohesion': 70, 'reference': 275, 'resolution': 283, 'coherence': 69, 'structure': 319, 'naturallanguage': 228, 'architecture': 35, 'nlg': 233, 'systems': 331, 'problems': 261, 'evaluation': 124, 'characteristics': 56, 'indian': 174, 'languages': 193, 'information': 175, 'retrieval': 286, 'resources': 284, 'design': 96, 'features': 135, 'classical': 63, 'non': 236, 'alternative': 23, 'valuation': 360, 'embeddings': 116, 'word2vec': 369, 'glove': 149, 'graphical': 154, 'sequence': 299, 'labelling': 190, 'lab': 188, 'exercises': 129, '30': 12, 'write': 374, 'program': 264, 'tokenize': 342, 'count': 82, 'frequency': 144, 'remove': 278, 'stop': 317, 'english': 119, 'synonyms': 328, 'wordnet': 370, 'antonyms': 28, 'stemming': 315, 'lemmatizing': 195, 'differentiate': 102, 'pos': 255, '10': 1, 'based': 42, 'ibm': 166, 'essential': 121, 'reading': 269, 'daniel': 89, 'jurafsky': 182, 'james': 181, '2nd': 11, 'edition': 109, 'martin': 208, 'prentice': 258, 'hall': 156, '2013': 8, 'foundations': 143, 'statistical': 313, 'cambridge': 51, 'ma': 201, 'mit': 222, 'press': 259, '1999': 6, 'recommended': 274, 'linguistics': 198, 'computer': 75, 'communication': 71, 'roland': 287, 'hausser': 159, 'springer': 310, '2014': 9, 'steven': 316, 'bird': 47, 'ewan': 125, 'klein': 186, 'edward': 111, 'loper': 199, 'python': 267, 'reilly': 277, 'media': 216, '2009': 7, 'web': 364, 'https': 164, 'stanford': 311, 'edu': 110, 'slp3': 304, 'ed3book': 107, 'pdf': 248, 'nptel': 238, 'ac': 16, 'courses': 85, '106101007': 2, 'tool': 344, 'kit': 185, 'http': 163, 'www': 375, 'org': 241, 'candidates': 54, 'having': 160, '50': 13, 'aggregate': 20, 'recognised': 271, 'university': 355, 'india': 173, 'abroad': 15, 'ugc': 350, 'aiu': 21, 'following': 141, 'programmes': 265, 'eligible': 115, 'bsc': 49, 'tech': 335, 'mathematics': 210, 'major': 204, 'minor': 220, 'ii': 168, 'bcom': 46, 'bba': 44, 'business': 50, 'maths': 211, 'data': 90, 'analytics': 26, 'specialization': 307, 'candidate': 53, 'passed': 247, 'undergraduate': 351, 'degree': 92, 'bca': 45, 'science': 290, 'students': 320, 'fulfill': 145, 'criteria': 88, 'described': 95, 'order': 240, 'bachelor': 40, 'graduate': 151, 'subjects': 325, 'statistics': 314, 'appearing': 29, 'final': 139, 'examinations': 127, 'march': 206, '2022': 10, 'applicants': 30, 'year': 376, 'studies': 322, 'secured': 291, 'semesters': 296, 'years': 377, 'conducted': 77, 'far': 134, 'minimum': 219, 'studied': 321, 'pre': 257, 'siamese': 300, 'network': 230, 'neural': 232, 'similarity': 303, 'state': 312, 'art': 36, 'deep': 91, 'achieve': 17, 'exceptional': 128, 'image': 169, 'recognition': 272, 'segmentation': 292, 'results': 285, 'exhibit': 130, 'great': 155, 'efficiency': 112, 'require': 281, 'tons': 343, 'labelled': 189, 'wonder': 367, 'massive': 209, 'scale': 289, 'available': 38, 'avoid': 39, 'poorly': 253, 'performing': 249, 'lack': 191, 'inaccurate': 171, 'model': 223, 'efficient': 113, 'working': 372, 'small': 305, 'demand': 93, 'came': 52, 'picture': 252, 'twin': 349, 'supervised': 327, 'technique': 336, 'introduced': 179, '1994': 5, 'widely': 366, 'deployed': 94, 'heterogeneous': 161, 'fields': 138, 'recognize': 273, 'times': 341, 'forgery': 142, 'inputs': 177, 'class': 61, 'networks': 231, 'famous': 133, 'containing': 79, 'identical': 167, 'sub': 324, 'considered': 78, 'mirrored': 221, 'weights': 365, 'parameters': 245, 'works': 373, 'tandem': 333, 'different': 101, 'input': 176, 'vectors': 362, 'compute': 74, 'comparable': 72, 'output': 243, 'mainly': 203, 'check': 57, 'measure': 215, 'use': 357, 'functions': 146, 'contrastive': 80, 'loss': 200, 'triplet': 348, 'circle': 60, 'train': 346, 'doing': 106, 'determines': 99, 'profiles': 263, 'similar': 302, 'instances': 178, 'positive': 256, 'negative': 229, 'generated': 147, 'thought': 339, 'projected': 266, 'representations': 280, 'employed': 118, 'unique': 353, 'approach': 33, 'finding': 140, 'additional': 18, 'advantage': 19, 'requiring': 282, 'fewer': 136, 'samples': 288, 'efficiently': 114, 'imbalanced': 170, 'striking': 318, 'field': 137, 'audio': 37, 'signal': 301, 'chromosomes': 59, 'karyotyping': 183, 'classifications': 65, 'chemistry': 58, 'pharmacology': 251, 'handwritten': 157, 'kinship': 184, 'verification': 363, 'counterfeit': 83, 'detecting': 97, 'questions': 268, 'portals': 254, 'examination': 126}\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "tfidfvectorizer.fit(w)\n",
    "print(tfidfvectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7556c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>106101007</th>\n",
       "      <th>12</th>\n",
       "      <th>150</th>\n",
       "      <th>1994</th>\n",
       "      <th>1999</th>\n",
       "      <th>2009</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>words</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>write</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       05   10  106101007   12  150  1994  1999  2009  2013  2014  ...  word  \\\n",
       "0     0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1     0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2     0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3     0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4     0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...   ...  ...        ...  ...  ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "1425  0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1426  0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1427  0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1428  0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1429  0.0  0.0        0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "      word2vec  wordnet  words  working  works  write  www  year  years  \n",
       "0          0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "1          0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "2          0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "3          0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "4          0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "...        ...      ...    ...      ...    ...    ...  ...   ...    ...  \n",
       "1425       0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "1426       0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "1427       0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "1428       0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "1429       0.0      0.0    0.0      0.0    0.0    0.0  0.0   0.0    0.0  \n",
       "\n",
       "[1430 rows x 378 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_wm = tfidfvectorizer.fit_transform(w)\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4903bcb",
   "metadata": {},
   "source": [
    "### Comparing TF-IDF & CountVectorizer\n",
    "\n",
    "TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
